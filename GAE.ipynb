{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANV_beJjV0ix"
      },
      "source": [
        "\n",
        "## Generalized Advantage Estimation in Reinforcement Learning\n",
        "\n",
        "*****************************************************************************\n",
        "### References:\n",
        "[1] Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.\n",
        "\n",
        "\n",
        "[2] The code in this notebook demo is adapted based on - https://colab.research.google.com/drive/1Wb_2zKgAqhI2tVK19Y1QC8AHImrzlcme?usp=sharing\n",
        "\n",
        "*****************************************************************************\n",
        "*Comments of this notebook to be added later*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UultlBt4Wh1g"
      },
      "source": [
        "**Some Transition Dynamics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEfCUb5vc8NW"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3vvrP5nT7Hi"
      },
      "source": [
        "P = np.array([[[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [1, 0, 0, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 1, 0, 0, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_3           |-> for action a_0 i.e. left          \n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]],   # s_6 terminal /\n",
        "              \n",
        "              [[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_3           |-> for action a_1 i.e. right\n",
        "               [0, 0, 0, 0, 0, 1, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 0, 0, 1],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]]])  # s_6 terminal /\n",
        "#  State:       0  1  2  3  4  5  6\n",
        "\n",
        "r = np.array([[0, 0],  # s_0\n",
        "              [0, 0],  # s_1\n",
        "              [0, 0],  # s_2\n",
        "              [0, 0],  # s_3\n",
        "              [0, 0],  # s_4\n",
        "              [0, 1],  # s_5\n",
        "              [0, 0]]) # s_6\n",
        "# Action:    a_0  a_1\n",
        "\n",
        "pi = np.array([[0.5, 0.5],  # s_0\n",
        "               [0.5, 0.5],  # s_1\n",
        "               [0.5, 0.5],  # s_2\n",
        "               [0.5, 0.5],  # s_3\n",
        "               [0.5, 0.5],  # s_4\n",
        "               [0.5, 0.5],  # s_5\n",
        "               [0.5, 0.5]]) # s_6\n",
        "# Action:       a_0  a_1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9mLnOpKW17N"
      },
      "source": [
        "Given a linear chain, the step function will calculate the next stochastic state and reward automatically based on the action and the transition dynamics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSevAQCgWssX"
      },
      "source": [
        "class LinearChain():\n",
        "    def __init__(self, P, r, start_state, terminal_states, noise=0):\n",
        "        self.P = P\n",
        "        self.r = r\n",
        "        self.noise = noise\n",
        "        self.n = P.shape[-1]\n",
        "        self.start_state = start_state\n",
        "        self.terminal_states = terminal_states\n",
        "\n",
        "        self.observation_space = self.n\n",
        "        self.action_space = 2\n",
        "        self.state = None\n",
        "\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state is None:\n",
        "            raise Exception('step() used before calling reset()')\n",
        "        assert action in range(P.shape[0])\n",
        "\n",
        "        reward = r[self.state, action] \\\n",
        "            + np.random.normal(loc=0, scale=self.noise)\n",
        "        self.state = np.random.choice(a=self.n, p=self.P[action, self.state])\n",
        "        self.t = self.t + 1\n",
        "\n",
        "        done = False\n",
        "        if self.state in self.terminal_states:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRpkDkqWcuLO"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, num_actions, policy_features, value_features,\n",
        "                 policy_stepsize, value_stepsize, nstep, lambda_, gamma,\n",
        "                 FLAG_BASELINE, FLAG_POPULAR_PG=False):\n",
        "        self.policy_features = policy_features\n",
        "        self.value_features = value_features\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.policy_weight = np.zeros((policy_features.shape[1],\n",
        "                                       num_actions))\n",
        "        if value_features is None:\n",
        "            self.value_weight = None\n",
        "        else:\n",
        "            self.value_weight = np.zeros((value_features.shape[1], 1))\n",
        "\n",
        "        self.policy_stepsize = policy_stepsize\n",
        "        self.value_stepsize = value_stepsize\n",
        "\n",
        "        self.FLAG_BASELINE = FLAG_BASELINE\n",
        "        self.FLAG_POPULAR_PG = FLAG_POPULAR_PG\n",
        "        self.gamma = gamma\n",
        "        # Parameter for calculating the generalized advantage.\n",
        "        self.lambda_ = lambda_\n",
        "        self.nstep = nstep\n",
        "\n",
        "        self.pi = None\n",
        "        self.FLAG_POLICY_UPDATED = True\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x, 1).reshape(-1, 1))\n",
        "        out = e_x / e_x.sum(1).reshape(-1, 1)\n",
        "        return out\n",
        "\n",
        "    # At a given state, use the existing stochastic policy to decide which \n",
        "    # action to take.\n",
        "    def take_action(self, state):\n",
        "        if self.FLAG_POLICY_UPDATED:\n",
        "            action_prefs = np.matmul(self.policy_features, self.policy_weight)\n",
        "            self.pi = self.softmax(action_prefs)\n",
        "            self.FLAG_POLICY_UPDATED = False\n",
        "            \n",
        "        action = np.random.choice(self.num_actions, p=self.pi[state])\n",
        "        return action, self.pi[state, action]\n",
        "\n",
        "    # Use the current value functions to make predictions.\n",
        "    def calc_v_pi_pred(self):\n",
        "        return np.matmul(self.value_features, self.value_weight)\n",
        "\n",
        "    # =========================================================================\n",
        "    # Calculate the advantage for a specific step.\n",
        "    def calc_advantage(self, curr_state, next_state, reward):\n",
        "        return reward + (next_state @ self.value_weight - \\\n",
        "                         curr_state @ self.value_weight)\n",
        "\n",
        "    # After taking n steps forward and getting the trajectory,  calculate the \n",
        "    # advantage at each timestep t. \n",
        "    def calc_generalized_advantage(self, t, traj, v_pi):\n",
        "        reward_list = traj['reward_list']\n",
        "        next_state_list = traj['next_state_list']\n",
        "        traj_length = len(reward_list)\n",
        "\n",
        "        nstep = self.nstep\n",
        "        assert nstep  == 'inf' or nstep > 0\n",
        "        if nstep == 'inf' or nstep > traj_length:\n",
        "            nstep = traj_length\n",
        "\n",
        "        GAE = 0\n",
        "        discount = 1\n",
        "        for i in range(t, min(t+nstep, traj_length)):\n",
        "            GAE += discount * self.calc_advantage(next_state_list[i], \n",
        "                                                  next_state_list[i+1], \n",
        "                                                  reward_list[i])\n",
        "            discount *= self.gamma * self.lambda_\n",
        "        # i = min(t+nstep, traj_length) - 1\n",
        "        # nstep_return += discount * v_pi[next_state_list[i]]\n",
        "            \n",
        "        return GAE\n",
        "    # =========================================================================\n",
        "    \n",
        "    # # Using the difference between the accumulated n-step reward and the estimated\n",
        "    # # value predicted by the value function, update the value function parameter.\n",
        "    # def update_value_fn(self, traj):\n",
        "    #     state_list = traj['state_list']\n",
        "    #     traj_length = len(state_list)\n",
        "\n",
        "    #     for t in range(traj_length):\n",
        "    #         state = state_list[t]\n",
        "    #         v_pi = self.calc_v_pi_pred()\n",
        "    #         G = self.calc_nstep_return(t, traj, v_pi)\n",
        "                    \n",
        "    #         v_pred = v_pi[state]\n",
        "    #         self.value_weight = self.value_weight \\\n",
        "    #             + self.value_stepsize * (G - v_pred) \\\n",
        "    #             * self.value_features[state].reshape(self.value_weight.shape)\n",
        "\n",
        "    # # helper function for calculating the policy gradient.\n",
        "    # def calc_grad_log_pi(self, state, action):\n",
        "    #     x = self.policy_features[state].reshape(-1, 1)\n",
        "    #     action_prefs = np.matmul(x.T, self.policy_weight)\n",
        "    #     pi = self.softmax(action_prefs).T\n",
        "\n",
        "    #     I_action = np.zeros((self.num_actions, 1))\n",
        "    #     I_action[action] = 1\n",
        "\n",
        "    #     one_vec = np.ones((1, self.num_actions))\n",
        "\n",
        "    #     return np.matmul(x, one_vec) * (I_action - pi).T\n",
        "\n",
        "    # # Calculate the REINFORCE based policy gradient.\n",
        "    # def calc_reinforce_pg(self, traj, v_pi):\n",
        "    #     state_list = traj['state_list']\n",
        "    #     action_list = traj['action_list']\n",
        "    #     traj_length = len(state_list)\n",
        "        \n",
        "    #     policy_grad = np.zeros(self.policy_weight.shape)\n",
        "    #     for t in range(traj_length):\n",
        "    #         state = state_list[t]\n",
        "    #         action = action_list[t]\n",
        "    #         G = self.calc_nstep_return(t, traj, v_pi)\n",
        "    #         grad_log_pi = self.calc_grad_log_pi(state, action)\n",
        "            \n",
        "    #         if self.FLAG_BASELINE:\n",
        "    #             baseline = v_pi[state]\n",
        "    #         else:\n",
        "    #             baseline = 0\n",
        "\n",
        "    #         if self.FLAG_POPULAR_PG == False:\n",
        "    #             policy_grad += self.gamma**t * (G - baseline) * grad_log_pi\n",
        "    #         else:\n",
        "    #             policy_grad += (G - baseline) * grad_log_pi\n",
        "\n",
        "    #     return policy_grad\n",
        "\n",
        "    # # Use the policy gradient to update the policy function.\n",
        "    # def update_policy(self, traj, v_pi):\n",
        "    #     policy_grad = self.calc_reinforce_pg(traj, v_pi)\n",
        "        \n",
        "    #     self.policy_weight = self.policy_weight \\\n",
        "    #         + self.policy_stepsize * policy_grad\n",
        "\n",
        "    #     self.FLAG_POLICY_UPDATED = True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}