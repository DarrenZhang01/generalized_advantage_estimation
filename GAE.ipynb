{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANV_beJjV0ix"
      },
      "source": [
        "\n",
        "## Generalized Advantage Estimation in Reinforcement Learning\n",
        "\n",
        "### References:\n",
        "[1] Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.\n",
        "\n",
        "\n",
        "The code in this notebook demo is adapted based on - https://colab.research.google.com/drive/1Wb_2zKgAqhI2tVK19Y1QC8AHImrzlcme?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_TJljm0HJcT"
      },
      "source": [
        "#### **Problem Setting**\n",
        "\n",
        "In this notebook, we consider a typical formulation of the policy optimization problem. The initial state $s_0$ is sampled from initial distribution $p_0$. A trajectory $(s_0, a_0, s_1, s_0, ...)$ is generated by sampling from policy $\\pi(a_t|s_t)$. The states then transition from $s_t$ to $s_{t+1}$ from a sampled distribution of $P(s_{t+1}|s_t, a_t)$. A reward $r_t = r(s_t, a_t, s_{t+1})$ is dispensed at each time stamp. The goal is to maximize the expected total reward $\\sum_{t=0}^{\\infty}r_t$, which is assumed to be finite for all policies in this problem setting. \n",
        "\n",
        "Moreover, in this problem, we are not assuming a discount as part of the problem specification. It appears to be a controllable parameter that adjusts bias-variance tradeoff.\n",
        "\n",
        "Policy gradient methods maximizes the expected total reward by estimating the policy gradient at each time stamp. The gradient which points to the direction where parameter $\\theta$ in the policy term $\\pi_\\theta(a_t|s_t)$ is hard to be computed through a closed-form expression. We aim to demonstrate a method of estimating the policy gradient empirically through a set of trajectories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSziO8GHJR9"
      },
      "source": [
        "#### **Policy Gradient**\n",
        "\n",
        "We can be represented by\n",
        "\n",
        "$$ g = \\triangledown_{\\theta} E[\\sum_{t=0}^{\\infty} r_t]. $$\n",
        "\n",
        "The estimation of policy gradient through a set of trajectories is one of the most fundamental problems. In this problem settig, we assume that the policy gradient possess the following particular form:\n",
        "\n",
        "$$ g = E[\\sum_{t=0}^{\\infty} A^{\\pi}(s_t, a_t) \\triangledown_\\theta log \\pi_{\\theta}(a_t | s_t)], $$\n",
        "\n",
        "where $A^{\\pi}$ is the advantage of $(a_t, s_t)$ in comparisons to the average value function $V^{\\pi}(s_t) = E_{s_{t+1}:\\infty, a_t:\\infty}[\\sum_{l=0}^{\\infty}r_{t+l}]$.\n",
        "\n",
        "That is, $A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t, a_t),$\n",
        "\n",
        "where $Q^{\\pi}(s_t, a_t) = E_{s_{t+1}:\\infty, a_{t+1}:\\infty}[\\sum_{l=0}^{\\infty}r_{t+l}]$. \n",
        "\n",
        "The difference between $Q^{\\pi}(s_t, a_t)$ and $V^{\\pi}(s_t)$ is that $Q^{\\pi}(s_t, a_t)$ the expected return across all rollouts after taking action $a_t$ at $s_t$ whereas $V^{\\pi}(s_t)$ is the expected return across all actions taken in $s_t$ resulting all subsequential rollouts. The advantage function is therefore the \"advantage\" of taking a specific action $a_t$ at $s_t$ in comparison to the average overall return of all actions to be taken at $s_t$.\n",
        "\n",
        "The selection of $A^{\\pi}(s_t, a_t)$ term as part of the policy gradient ensures that the gradient points in the direction of increased $\\pi_{\\theta}(a_t|s_t)$ if and only if $A^{\\pi}(s_t, a_t) > 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdCSdKOYl9rr"
      },
      "source": [
        "#### **Approximation of Policy Gradient**\n",
        "\n",
        "We can introduce a parameter $\\gamma$ in the estimation of $g$, denoted as $g^{\\gamma}$, to control the bias-variance tradeoff in the estimation.\n",
        "\n",
        "$$ g^{\\gamma} = E[\\sum_{t=0}^{\\infty} A^{\\pi, \\gamma}(s_t, a_t) \\triangledown_\\theta log \\pi_{\\theta}(a_t | s_t)], $$\n",
        "\n",
        "where $A^{\\pi, \\gamma}(s_t, a_t)$ is the discounted advantage function, which has form as\n",
        "\n",
        "$$A^{\\pi, \\gamma}(s_t, a_t) = Q^{\\pi, \\gamma}(s_t, a_t) + V^{\\pi, \\gamma}(s_t).$$\n",
        "\n",
        "$Q^{\\pi, \\gamma}(s_t, a_t), V^{\\pi, \\gamma}(s_t)$ are the discounted version of $Q^{\\pi}(s_t, a_t), V^{\\pi}(s_t)$, i.e.,\n",
        "\n",
        "$$Q^{\\pi, \\gamma}(s_t, a_t) = E_{s_{t+1:\\infty}, a_{t+1:\\infty}}[\\sum_{l=0}^\\infty\\gamma^l r_{t+l}]$$ \n",
        "\n",
        ",and\n",
        "\n",
        "$$V^{\\pi, \\gamma}(s_t) = E_{s_{t+1:\\infty}, a_{t:\\infty}}[\\sum_{l=0}^\\infty\\gamma^l r_{t+l}].$$\n",
        "\n",
        "\n",
        "They have the exact same form as in the literature of discounted return in Reinforcement Learning. However, in this context, the purpose of the parameter $\\gamma$ is to control the variability of value function at any state $s_t$ by introducing bias term.\n",
        "\n",
        "Based on the above estimator $g^{\\gamma}$, we want to estimate $g^{\\gamma}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc0cht1YHJo0"
      },
      "source": [
        "#### **Generalized Estimator of Advange**\n",
        "\n",
        "The target of Generalized Estimator of Advantage is to have\n",
        "\n",
        "$$\\hat{A}_t(s_{0:\\infty}, a_{0;\\infty})$$\n",
        "\n",
        "such that \n",
        "\n",
        "$$E_{s_0:\\infty,a_0:\\infty}[\\sum_{t=0}^{\\infty}\\hat{A}_t(s_{0:\\infty}, a_{0;\\infty})\\triangledown_\\theta log \\pi_{\\theta}(a_t | s_t)] = E[\\sum_{t=0}^{\\infty} A^{\\pi, \\gamma}(s_t, a_t) \\triangledown_\\theta log \\pi_{\\theta}(a_t | s_t)].$$\n",
        "\n",
        "Note that we already introduced bias to the estimation of policy gradient when we replace $A^{\\pi}$ by $A^{\\pi, \\gamma}$. Here we only consider an unbiased estimator of $g^{\\gamma}$, which is a biased estimate of policy gradient of undiscounted MDP.\n",
        "\n",
        "Suppose that $V$ is an approximate value function, we define TD residuals $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$ where $\\gamma$ is the discount factor. $\\delta_t^V$ can be considered as an estimate of the advantage of the action $a_t$. If we have the correct value function $V = V^{\\pi, \\gamma}$, we can easily show that \n",
        "\n",
        "$$E_{s_{t+1}}[\\delta_t^{V^{\\pi, \\gamma}}] = E_{s_{t+1}}[Q^{\\pi,\\gamma}(s_t,a_t) - V^{\\pi, \\gamma}(s_t)] = A^{\\pi, \\gamma}(s_t, a_t).$$\n",
        "\n",
        "However, it is not unbiased estimator if $V \\neq V^{\\pi, \\gamma}$, which means that using TD residual would introduce bias unless we know the exact value function.\n",
        "\n",
        "Consider a general $k$ sums of these TD residuals,\n",
        "\n",
        "$$\\hat{A}_t^{(1)} = \\delta_t^V = -V(s_t) + r_t + \\gamma V(s_{t+1})$$\n",
        "$$\\hat{A}_t^{(2)} = \\delta_t^V + \\gamma \\delta_{t+1}^V = -V(s_t) + r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+1})$$\n",
        "$$\\hat{A}_t^{(k)} = \\sum_{l=0}^{k-1} \\gamma^l \\delta_{t+l}^V = -V(s_t) + r_t + \\gamma r_{t+1} + ... + \\gamma^{k-1} r_{t+k-1} + \\gamma^kV(s_{t+k})$$\n",
        "\n",
        "An importan obervasion is that as $k$ becomes larger, the bias introduced by inaccurate value function $V$ gets smaller and smaller. As $k \\rightarrow \\infty$,\n",
        "\n",
        "$$\\hat{A}_t^{\\infty} = \\sum_{l=0}^{\\infty} \\gamma^l \\delta_{t+l}^V = -V(s_t) + \\sum_{l=0}^{\\infty} \\gamma^l r_{t+l}.$$\n",
        "\n",
        "Recall that $A^{\\pi, \\gamma} = Q^{\\pi, \\gamma}(s_t,a_t) - V^{\\pi, \\gamma}(s_t) = E_{s_{t+1:\\infty}, a_{t+1:\\infty}}[\\sum_{l=0}^{\\infty} \\gamma^l r_{t+l}] - V^{\\pi, \\gamma}(s_t)$.\n",
        "\n",
        "Comparing these two above equations, we can find that taking expectation of $\\hat{A}_t^{\\infty}$ would result in expectation of $A^{\\pi, \\gamma}$, regardless of the choice of $V$ by Proposition 1 in the paper.\n",
        "\n",
        "**Prop. 1** Suppose that $\\hat{A}_t$ can be written in the form $\\hat{A}_t(s_{0:\\infty},a_{0:\\infty}) = Q_t(s_{t:\\infty}, a_{t:\\infty}) - b_t(s_{0:t}, a_{0:{t-1}})$ such that for all $(s_t, a_t)$, $E_{s_{t+1:\\infty},a_{t+1:\\infty}}[Q_t(s_{t:\\infty},a_{t:\\infty})] = Q^{\\pi, \\gamma}(s_t, a_t). Then, E_{s_{0:\\infty},a_{0:\\infty}}[\\hat{A}_t(s_{0:\\infty},a_{0:\\infty})] = E_{s_{0:\\infty},a_{0:\\infty}}[A^{\\pi, \\gamma}(s_t,a_t)]$.\n",
        "\n",
        "To further control the bias-varaince tradeoff with estimator $A^{\\infty}_t$\n",
        "Therefore, an enhancement of the above estimator is to use exponentially-weighted sum of $k$-step estimator:\n",
        "\n",
        "$$\\hat{A}^{GAE(\\gamma,\\lambda)} = (1-\\lambda)(\\hat{A}^{(1)}_t + \\lambda \\hat{A}^{(2)}_t + ...) = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}^V$$\n",
        "\n",
        "Note that when $\\lambda = 0$, $\\hat{A}^{GAE(\\gamma,0)}$ is reduced to $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$, which as discussed above is biased unless $V = V^{\\pi, \\gamma}$. When $\\lambda = 1$, $\\hat{A}^{GAE(\\gamma,0)} = A_t^{\\infty}$ which has expectation equal to the expectation of $A^{\\pi, \\gamma}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UultlBt4Wh1g"
      },
      "source": [
        "**Some Transition Dynamics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3vvrP5nT7Hi"
      },
      "source": [
        "P = np.array([[[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [1, 0, 0, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 1, 0, 0, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_3           |-> for action a_0 i.e. left          \n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]],   # s_6 terminal /\n",
        "              \n",
        "              [[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_3           |-> for action a_1 i.e. right\n",
        "               [0, 0, 0, 0, 0, 1, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 0, 0, 1],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]]])  # s_6 terminal /\n",
        "#  State:       0  1  2  3  4  5  6\n",
        "\n",
        "r = np.array([[0, 0],  # s_0\n",
        "              [0, 0],  # s_1\n",
        "              [0, 0],  # s_2\n",
        "              [0, 0],  # s_3\n",
        "              [0, 0],  # s_4\n",
        "              [0, 1],  # s_5\n",
        "              [0, 0]]) # s_6\n",
        "# Action:    a_0  a_1\n",
        "\n",
        "pi = np.array([[0.5, 0.5],  # s_0\n",
        "               [0.5, 0.5],  # s_1\n",
        "               [0.5, 0.5],  # s_2\n",
        "               [0.5, 0.5],  # s_3\n",
        "               [0.5, 0.5],  # s_4\n",
        "               [0.5, 0.5],  # s_5\n",
        "               [0.5, 0.5]]) # s_6\n",
        "# Action:       a_0  a_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9mLnOpKW17N"
      },
      "source": [
        "Given a linear chain, the step function will calculate the next stochastic state and reward automatically based on the action and the transition dynamics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSevAQCgWssX"
      },
      "source": [
        "class LinearChain():\n",
        "    def __init__(self, P, r, start_state, terminal_states, noise=0):\n",
        "        self.P = P\n",
        "        self.r = r\n",
        "        self.noise = noise\n",
        "        self.n = P.shape[-1]\n",
        "        self.start_state = start_state\n",
        "        self.terminal_states = terminal_states\n",
        "\n",
        "        self.observation_space = self.n\n",
        "        self.action_space = 2\n",
        "        self.state = None\n",
        "\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state is None:\n",
        "            raise Exception('step() used before calling reset()')\n",
        "        assert action in range(P.shape[0])\n",
        "\n",
        "        reward = r[self.state, action] \\\n",
        "            + np.random.normal(loc=0, scale=self.noise)\n",
        "        self.state = np.random.choice(a=self.n, p=self.P[action, self.state])\n",
        "        self.t = self.t + 1\n",
        "\n",
        "        done = False\n",
        "        if self.state in self.terminal_states:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKjJq4Mo8kcw"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, num_actions, policy_features, value_features,\n",
        "                 policy_stepsize, value_stepsize, nstep, lambda_, gamma,\n",
        "                 FLAG_BASELINE, FLAG_POPULAR_PG=False):\n",
        "        self.policy_features = policy_features\n",
        "        self.value_features = value_features\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.policy_weight = np.zeros((policy_features.shape[1],\n",
        "                                       num_actions))\n",
        "        if value_features is None:\n",
        "            self.value_weight = None\n",
        "        else:\n",
        "            self.value_weight = np.zeros((value_features.shape[1], 1))\n",
        "\n",
        "        self.policy_stepsize = policy_stepsize\n",
        "        self.value_stepsize = value_stepsize\n",
        "\n",
        "        self.FLAG_BASELINE = FLAG_BASELINE\n",
        "        self.FLAG_POPULAR_PG = FLAG_POPULAR_PG\n",
        "        self.gamma = gamma\n",
        "        # Parameter for calculating the generalized advantage.\n",
        "        self.lambda_ = lambda_\n",
        "        self.nstep = nstep\n",
        "\n",
        "        self.pi = None\n",
        "        self.FLAG_POLICY_UPDATED = True\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x, 1).reshape(-1, 1))\n",
        "        out = e_x / e_x.sum(1).reshape(-1, 1)\n",
        "        return out\n",
        "\n",
        "    # At a given state, use the existing stochastic policy to decide which \n",
        "    # action to take.\n",
        "    def take_action(self, state):\n",
        "        if self.FLAG_POLICY_UPDATED:\n",
        "            action_prefs = np.matmul(self.policy_features, self.policy_weight)\n",
        "            self.pi = self.softmax(action_prefs)\n",
        "            self.FLAG_POLICY_UPDATED = False\n",
        "            \n",
        "        action = np.random.choice(self.num_actions, p=self.pi[state])\n",
        "        return action, self.pi[state, action]\n",
        "\n",
        "    # Use the current value functions to make predictions.\n",
        "    def calc_v_pi_pred(self):\n",
        "        return np.matmul(self.value_features, self.value_weight)\n",
        "\n",
        "    # =========================================================================\n",
        "    # Calculate the advantage for a specific step.\n",
        "    def calc_advantage(self, curr_state, next_state, reward):\n",
        "        return reward + (next_state @ self.value_weight - \\\n",
        "                         curr_state @ self.value_weight)\n",
        "\n",
        "    # After taking n steps forward and getting the trajectory,  calculate the \n",
        "    # advantage at each timestep t. \n",
        "    def calc_generalized_advantage(self, t, traj, v_pi):\n",
        "        reward_list = traj['reward_list']\n",
        "        next_state_list = traj['next_state_list']\n",
        "        traj_length = len(reward_list)\n",
        "\n",
        "        nstep = self.nstep\n",
        "        assert nstep  == 'inf' or nstep > 0\n",
        "        if nstep == 'inf' or nstep > traj_length:\n",
        "            nstep = traj_length\n",
        "\n",
        "        GAE = 0\n",
        "        discount = 1\n",
        "        for i in range(t, min(t+nstep, traj_length)):\n",
        "            GAE += discount * self.calc_advantage(next_state_list[i], \n",
        "                                                  next_state_list[i+1], \n",
        "                                                  reward_list[i])\n",
        "            discount *= self.gamma * self.lambda_\n",
        "        # i = min(t+nstep, traj_length) - 1\n",
        "        # nstep_return += discount * v_pi[next_state_list[i]]\n",
        "            \n",
        "        return GAE\n",
        "    # =========================================================================\n",
        "    \n",
        "    # # Using the difference between the accumulated n-step reward and the estimated\n",
        "    # # value predicted by the value function, update the value function parameter.\n",
        "    # def update_value_fn(self, traj):\n",
        "    #     state_list = traj['state_list']\n",
        "    #     traj_length = len(state_list)\n",
        "\n",
        "    #     for t in range(traj_length):\n",
        "    #         state = state_list[t]\n",
        "    #         v_pi = self.calc_v_pi_pred()\n",
        "    #         G = self.calc_nstep_return(t, traj, v_pi)\n",
        "                    \n",
        "    #         v_pred = v_pi[state]\n",
        "    #         self.value_weight = self.value_weight \\\n",
        "    #             + self.value_stepsize * (G - v_pred) \\\n",
        "    #             * self.value_features[state].reshape(self.value_weight.shape)\n",
        "\n",
        "    # # helper function for calculating the policy gradient.\n",
        "    # def calc_grad_log_pi(self, state, action):\n",
        "    #     x = self.policy_features[state].reshape(-1, 1)\n",
        "    #     action_prefs = np.matmul(x.T, self.policy_weight)\n",
        "    #     pi = self.softmax(action_prefs).T\n",
        "\n",
        "    #     I_action = np.zeros((self.num_actions, 1))\n",
        "    #     I_action[action] = 1\n",
        "\n",
        "    #     one_vec = np.ones((1, self.num_actions))\n",
        "\n",
        "    #     return np.matmul(x, one_vec) * (I_action - pi).T\n",
        "\n",
        "    # # Calculate the REINFORCE based policy gradient.\n",
        "    # def calc_reinforce_pg(self, traj, v_pi):\n",
        "    #     state_list = traj['state_list']\n",
        "    #     action_list = traj['action_list']\n",
        "    #     traj_length = len(state_list)\n",
        "        \n",
        "    #     policy_grad = np.zeros(self.policy_weight.shape)\n",
        "    #     for t in range(traj_length):\n",
        "    #         state = state_list[t]\n",
        "    #         action = action_list[t]\n",
        "    #         G = self.calc_nstep_return(t, traj, v_pi)\n",
        "    #         grad_log_pi = self.calc_grad_log_pi(state, action)\n",
        "            \n",
        "    #         if self.FLAG_BASELINE:\n",
        "    #             baseline = v_pi[state]\n",
        "    #         else:\n",
        "    #             baseline = 0\n",
        "\n",
        "    #         if self.FLAG_POPULAR_PG == False:\n",
        "    #             policy_grad += self.gamma**t * (G - baseline) * grad_log_pi\n",
        "    #         else:\n",
        "    #             policy_grad += (G - baseline) * grad_log_pi\n",
        "\n",
        "    #     return policy_grad\n",
        "\n",
        "    # # Use the policy gradient to update the policy function.\n",
        "    # def update_policy(self, traj, v_pi):\n",
        "    #     policy_grad = self.calc_reinforce_pg(traj, v_pi)\n",
        "        \n",
        "    #     self.policy_weight = self.policy_weight \\\n",
        "    #         + self.policy_stepsize * policy_grad\n",
        "\n",
        "    #     self.FLAG_POLICY_UPDATED = True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}