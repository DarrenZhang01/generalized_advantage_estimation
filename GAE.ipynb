{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANV_beJjV0ix"
      },
      "source": [
        "\n",
        "## Generalized Advantage Estimation in Reinforcement Learning\n",
        "\n",
        "### References:\n",
        "[1] Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.\n",
        "\n",
        "\n",
        "The code in this notebook demo is adapted based on - https://colab.research.google.com/drive/1Wb_2zKgAqhI2tVK19Y1QC8AHImrzlcme?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UultlBt4Wh1g"
      },
      "source": [
        "**Some Transition Dynamics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3vvrP5nT7Hi"
      },
      "source": [
        "P = np.array([[[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [1, 0, 0, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 1, 0, 0, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_3           |-> for action a_0 i.e. left          \n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]],   # s_6 terminal /\n",
        "              \n",
        "              [[1, 0, 0, 0, 0, 0, 0],    # s_0 terminal \\\n",
        "               [0, 0, 1, 0, 0, 0, 0],    # s_1           |\n",
        "               [0, 0, 0, 1, 0, 0, 0],    # s_2           |\n",
        "               [0, 0, 0, 0, 1, 0, 0],    # s_3           |-> for action a_1 i.e. right\n",
        "               [0, 0, 0, 0, 0, 1, 0],    # s_4           |\n",
        "               [0, 0, 0, 0, 0, 0, 1],    # s_5           |\n",
        "               [0, 0, 0, 0, 0, 0, 1]]])  # s_6 terminal /\n",
        "#  State:       0  1  2  3  4  5  6\n",
        "\n",
        "r = np.array([[0, 0],  # s_0\n",
        "              [0, 0],  # s_1\n",
        "              [0, 0],  # s_2\n",
        "              [0, 0],  # s_3\n",
        "              [0, 0],  # s_4\n",
        "              [0, 1],  # s_5\n",
        "              [0, 0]]) # s_6\n",
        "# Action:    a_0  a_1\n",
        "\n",
        "pi = np.array([[0.5, 0.5],  # s_0\n",
        "               [0.5, 0.5],  # s_1\n",
        "               [0.5, 0.5],  # s_2\n",
        "               [0.5, 0.5],  # s_3\n",
        "               [0.5, 0.5],  # s_4\n",
        "               [0.5, 0.5],  # s_5\n",
        "               [0.5, 0.5]]) # s_6\n",
        "# Action:       a_0  a_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9mLnOpKW17N"
      },
      "source": [
        "Given a linear chain, the step function will calculate the next stochastic state and reward automatically based on the action and the transition dynamics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSevAQCgWssX"
      },
      "source": [
        "class LinearChain():\n",
        "    def __init__(self, P, r, start_state, terminal_states, noise=0):\n",
        "        self.P = P\n",
        "        self.r = r\n",
        "        self.noise = noise\n",
        "        self.n = P.shape[-1]\n",
        "        self.start_state = start_state\n",
        "        self.terminal_states = terminal_states\n",
        "\n",
        "        self.observation_space = self.n\n",
        "        self.action_space = 2\n",
        "        self.state = None\n",
        "\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state is None:\n",
        "            raise Exception('step() used before calling reset()')\n",
        "        assert action in range(P.shape[0])\n",
        "\n",
        "        reward = r[self.state, action] \\\n",
        "            + np.random.normal(loc=0, scale=self.noise)\n",
        "        self.state = np.random.choice(a=self.n, p=self.P[action, self.state])\n",
        "        self.t = self.t + 1\n",
        "\n",
        "        done = False\n",
        "        if self.state in self.terminal_states:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}